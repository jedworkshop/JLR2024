{
  "time": "15:30-17:05",
  "title": "事前学習モデルの構築と利用",
  "chair": "久保隆宏 (アマゾンウェブサービスジャパン)",
  "talks": [
    {
      "id": "d-1",
      "type": "normal",
      "time": "15:30-16:00",
      "title": "資源として見る実験プログラム",
      "presenter": "塚越駿 (名大)",
      "material": "https://drive.google.com/open?id=1Sw8AuTxBgawOGBmqS4lZdmQ0AFfF5oY_",
      "abstract": "深層学習を用いた自然言語処理において、言語資源は必要不可欠である。特に、モデルを訓練するための高品質なデータセットや、高性能な事前学習済み言語モデルの研究開発が活発に行われている。では、これらの言語資源があれば十分に研究を遂行できると言えるだろうか。実際には、円滑な研究遂行のために必要な資源はその他にも多く存在し、その中でも実験プログラムは、適切に妥当な実験を行うために欠かすことのできない重要な要素である。しかしその重要性は十分に認識されているとは言えず、高品質な実験プログラムと、その実装方針が共有されることは極めて少ない。本発表では、自然言語処理初学者向けに構築した「BERTによるテキスト分類チュートリアル」を中心に、自然言語処理の研究において実験プログラムが持つ役割と重要性について述べ、実験プログラム管理の方策と、自然言語処理を取り巻く近年の工学的発展について紹介する。"
    },
    {
      "id": "d-2",
      "type": "normal",
      "time": "16:00-16:20",
      "title": "日本語BigBirdの構築",
      "presenter": "近藤瑞希, 王昊, 井手竜也, 伊藤俊太朗, Ritvik Choudhary, 栗原健太郎, 河原大輔 (早大)",
      "material": "https://drive.google.com/open?id=1PgnQHMvxp5lqNZEESAF9LRfGR-ic_i-W",
      "abstract": "日本語の言語理解タスクを解くために,  日本語で事前学習されたモデルをファインチューニングすることで性能が向上することが知られている。日本語で事前学習されたモデルはいくつか公開されているが, ほぼすべてのモデルにおいて最大入力長は512トークン以下である。そのため, 日本語では長い入力長のデータに対して, 対応するモデルが存在せず適切な学習を行うことができない。この問題を解決するためにBigBird日本語版の事前学習を行った。BigBirdは最大で4096トークンの長さを扱うことができるEncoderモデルである。学習テキストとして日本語Wikipedia, CC-100, OSCARの3つを, バッチサイズ196, 60万ステップ(約10エポック)を目標として現在20万ステップ程学習した。このモデルを日本語言語理解ベンチマークJGLUEで評価した。また, 複数ノードで学習したときに得られた知見についても報告する。"
    },
    {
      "id": "d-3",
      "type": "lt",
      "time": "16:20-16:25",
      "title": "日本語DistilBERTの構築と性能評価",
      "presenter": "小林滉河, 李聖哲, 中町礼文 (LINE)",
      "material": "https://drive.google.com/open?id=1clQD9xy5_WcipMlsvdMUSyYstD8lbHBQ",
      "abstract": "近年、BERTやRoBERTaのような大規模言語モデルが自然言語処理におけるデファクトスタンダードになっている。しかし、これらのモデルはパラメーター数が多く、運用コストが高い。この問題を解決するため、軽量なモデルであるDistilBERTを大規模Webコーパスを用いて構築した。本発表では構築についての具体的な内容とJGLUEによる評価について報告する。"
    },
    {
      "id": "d-4",
      "type": "lt",
      "time": "16:25-16:30",
      "title": "日本語DeBERTaモデルの構築",
      "presenter": "植田暢大 (京大)",
      "material": "https://drive.google.com/open?id=1r9FxomK-LV7968_Y3lkCSND93v_7q-vC",
      "abstract": "本発表では，日本語DeBERTaモデルの事前学習および得られた知見について紹介する．我々は，文字レベルおよび単語レベルの tiny/base/large モデルを構築・公開した．学習には DeepSpeed ライブラリを使用しており，その結果得られた高速化や省メモリ化について種々の設定について比較する．また，マルチノード学習やハイパーパラメータ選択についても述べる．"
    },
    {
      "id": "d-5",
      "type": "panel",
      "time": "16:30-16:35",
      "title": "ディスカッション"
    },
    {
      "id": "d-6",
      "type": "normal",
      "time": "16:35-17:05",
      "title": "日本語事前学習モデルの構築およびビジネス応用の現状",
      "presenter": "佐藤敏紀 (LINE)",
      "material": "https://drive.google.com/open?id=17Xl2W-yal4xd6sYpZSJsYcUhb69aLInr",
      "abstract": "本発表では、様々な事前学習モデルを構築し応用した経験に基づいて、モデルの仕様と応用結果との関係性に言及し、それぞれのモデルのビジネス方面への活用に関する現在の見解を共有する。また、入力に対する最適解が決まりにくい対話システムの各種コンペティションにおいて、我々の実装が一定の成果をあげることができた要因について見解を紹介する。具体的には、マルチモーダル化や外部知識の参照、倫理フィルター、違法表現の検出、ファクトチェックなど様々な重厚なサブシステムを実装することの重要性について具体例を基にお伝えする。さらに、2023年度中にAPI公開を予定しているHyperCLOVAと著名なモデルとの比較結果についても可能な限り言及する。最後に我々がこれから取り組む事前学習モデルの構築と応用の方向性について言及し、コミュニティ全体の議論を促進したいと考えている。"
    }
  ]
}